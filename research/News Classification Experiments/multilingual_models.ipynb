{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e7e73d-d1f2-4f39-9c3d-51640100670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /root/anaconda3/lib/python3.9/site-packages (4.23.1)\n",
      "Requirement already satisfied: datasets in /root/anaconda3/lib/python3.9/site-packages (2.7.1)\n",
      "Requirement already satisfied: evaluate in /root/anaconda3/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /root/anaconda3/lib/python3.9/site-packages (0.12.1)\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /root/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /root/anaconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/lib/python3.9/site-packages (from huggingface-hub) (4.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /root/anaconda3/lib/python3.9/site-packages (from optuna) (1.4.22)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/anaconda3/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /root/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/anaconda3/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/root/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets evaluate huggingface-hub optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4262fd-8c72-47f0-b511-6e3d41095787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-02-25 23:34:07.710428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-25 23:34:07.828950: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-25 23:34:08.256683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-25 23:34:08.256760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-25 23:34:08.256766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, create_optimizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from optuna import trial\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecc9991-e8f3-4bf0-89a9-10d5005c47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837fc1ef-e529-4bee-9207-5591dbf32eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eb473d952f48af90b6a44386d670a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de84489-3106-42d5-b2a0-c39d5e52fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b0d047-c466-4093-bc04-5537cd504bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18882\n",
      "})\n",
      "Tokenizer loaded from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d76f66b18643e4b34d2010b11714b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a3a225e39f4cffb37803c89db066f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from huggingface\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"bert-base-multilingual-cased\"\n",
    "BASE_DATASET = \"valurank/News_Articles_Categorization\"\n",
    "LOCAL_DIR = \"./ml/\"\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = load_from_disk(LOCAL_DIR + \"/datasets/public_multilingual\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    # Combine science and tech category\n",
    "    label2id = {\"Politics\" : 2, \"Tech\": 3, \"Entertainment\": 0, \"Sports\": 1, \"Business\": 4, \"Health\" : 5, \"science\": 3}\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "\n",
    "    dataset = load_dataset('json', data_files=LOCAL_DIR + \"datasets/ml.json\", split=\"train\")\n",
    "    # Drop \"World\" category\n",
    "    dataset = dataset.filter(lambda i: i[\"label\"] != \"World\")\n",
    "    \n",
    "    features = dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    dataset = dataset.map(label_to_id, batched=True, features=features)\n",
    "    \n",
    "    dataset.save_to_disk(LOCAL_DIR + \"/datasets/public_multilingual\")\n",
    "    print(\"Loaded dataset from original file\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "# Split train and test\n",
    "train_test = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR + \"mbert/\", model_max_length=max_token_length)\n",
    "    print(\"Tokenizer loaded from local directory\")\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, model_max_length=max_token_length)\n",
    "    tokenizer.save_pretrained(LOCAL_DIR + \"mbert/\")\n",
    "    print(\"Tokenizer loaded from huggingface\")\n",
    "\n",
    "#From https://huggingface.co/docs/transformers/main/en/training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = train_test.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "print(\"Model loaded from huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974db2dc-5192-46f5-bdd2-2746f0cb2d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3fea2b759c30bfa1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3fea2b759c30bfa1/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed80a387194c1b8e21bd8611e0c5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e989cbcad3c43bcb8e54b0e81d0cd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3fea2b759c30bfa1/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2f64446e754d359715445703f2675c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9c82d440244e8eb49e3ef264454c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from original file\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 12846\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a0e4d7d753481ba762323e5d117b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c087084ec44d4d919515177d8cdeae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaba7c2da7d4c5e84d42c394eea7963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process collected data\n",
    "# Load dataset\n",
    "try:\n",
    "    collected_dataset = load_from_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "    label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "    collected_dataset = load_dataset('json', data_files=LOCAL_DIR + \"datasets/collected.json\", split=\"train\")\n",
    "    features = collected_dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    collected_dataset = collected_dataset.map(label_to_id, batched=True, features=features)\n",
    "    collected_dataset = collected_dataset.shuffle(seed=42)\n",
    "    \n",
    "    collected_dataset.save_to_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded dataset from original file\")\n",
    "\n",
    "print(collected_dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "full_collected_tokenized = collected_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split train and test\n",
    "collected_train_test = collected_dataset.train_test_split(test_size=test_split)\n",
    "collected_tokenized_dataset = collected_train_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fae45a-865c-4a3a-b95c-795dcbfde5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "# From https://huggingface.co/spaces/BucketHeadP65/confusion_matrix\n",
    "cfm = evaluate.load(\"BucketHeadP65/confusion_matrix\")\n",
    "\n",
    "# From https://huggingface.co/docs/transformers/main/en/tasks/sequence_classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    #confusion_matrix = cfm.compute(predictions=predictions, references=labels)[\"confusion_matrix\"]\n",
    "\n",
    "    return {\"precision\": precision_score, \"recall\": recall_score, \"f1\": f1_score,\n",
    "     \"accuracy\": accuracy_score\n",
    "            #,\"confusion_matrix\": confusion_matrix\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cab4ba-8daf-4aa3-97f9-42b44073b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://huggingface.co/docs/transformers/hpo_train\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 4e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4]),\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09921351-7452-4d04-8367-c851e10294bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-02-25 03:45:24,768]\u001b[0m A new study created in memory with name: no-name-9477fc01-40e5-4774-a648-363ef8ab9172\u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1892\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='1892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  36/1892 00:15 < 14:30, 2.13 it/s, Epoch 0.07/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_eng_model\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/mbert.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc5c1ab-314e-43ce-83ec-cffce4353ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 5e-05,\n",
       " 'per_device_train_batch_size': 32,\n",
       " 'num_train_epochs': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./ml/best_hparams/mbert.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/mbert_multilingual\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "best_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a97577-f325-4c03-8d5e-59c7c0670760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1419' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1419/1419 13:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.078673</td>\n",
       "      <td>0.974499</td>\n",
       "      <td>0.974318</td>\n",
       "      <td>0.974385</td>\n",
       "      <td>0.974318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.048178</td>\n",
       "      <td>0.985686</td>\n",
       "      <td>0.985703</td>\n",
       "      <td>0.985683</td>\n",
       "      <td>0.985703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.993389</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>0.993379</td>\n",
       "      <td>0.993381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-473\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-946\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-946/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-1419\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-1419/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-1419/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-1419/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-1419/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert_multilingual/checkpoint-1419 (score: 0.02315319888293743).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 799.859482 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f5f8c22f340>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "trainer.train()\n",
    "total_time_taken = (datetime.now() - start_time).total_seconds()\n",
    "print(\"Training took\", total_time_taken, \"seconds\")\n",
    "\n",
    "# Save model\n",
    "# trainer.save_model(LOCAL_DIR + \"trained_mbert_model/\")\n",
    "\n",
    "del(model)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf499103-c5ee-45e5-b57c-81d6c99c48a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1419' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1419/1419 13:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.094034</td>\n",
       "      <td>0.973973</td>\n",
       "      <td>0.973524</td>\n",
       "      <td>0.973665</td>\n",
       "      <td>0.973524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.052719</td>\n",
       "      <td>0.986899</td>\n",
       "      <td>0.986762</td>\n",
       "      <td>0.986797</td>\n",
       "      <td>0.986762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.049229</td>\n",
       "      <td>0.989216</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>0.989157</td>\n",
       "      <td>0.989145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-473\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-946\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-946/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-1419\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-1419/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-1419/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-1419/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-1419/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert_multilingual/checkpoint-1419 (score: 0.04922853782773018).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='521' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1419' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1419/1419 13:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.083955</td>\n",
       "      <td>0.975264</td>\n",
       "      <td>0.974848</td>\n",
       "      <td>0.974950</td>\n",
       "      <td>0.974848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.058481</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>0.981732</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.981732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>0.987080</td>\n",
       "      <td>0.987027</td>\n",
       "      <td>0.987035</td>\n",
       "      <td>0.987027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-473\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-946\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-946/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-1419\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-1419/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-1419/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-1419/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-1419/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert_multilingual/checkpoint-1419 (score: 0.04896368831396103).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='521' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1419' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1419/1419 13:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.083955</td>\n",
       "      <td>0.975264</td>\n",
       "      <td>0.974848</td>\n",
       "      <td>0.974950</td>\n",
       "      <td>0.974848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.058481</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>0.981732</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.981732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>0.987080</td>\n",
       "      <td>0.987027</td>\n",
       "      <td>0.987035</td>\n",
       "      <td>0.987027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-473\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-946\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-946/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert_multilingual/checkpoint-1419\n",
      "Configuration saved in ml/results/mbert_multilingual/checkpoint-1419/config.json\n",
      "Model weights saved in ml/results/mbert_multilingual/checkpoint-1419/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_multilingual/checkpoint-1419/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_multilingual/checkpoint-1419/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert_multilingual/checkpoint-1419 (score: 0.04896368831396103).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='521' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBert Accuracy: 98.7733% +/- 0.0998 Weighted F1: 98.7742% +/- 0.1001\n",
      "MBert Transferability: 66.0335% +/- 0.5101 Weighted F1: 65.9971% +/- 0.4781\n"
     ]
    }
   ],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "    mbert_results = trainer.evaluate(eval_dataset=full_collected_tokenized)\n",
    "    transferability_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"MBert Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"MBert Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f57d4c-7fe0-4701-bbac-b90d1ff8b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736b148187164f88af77ef2465448067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ff8521b084a9a9dcd0f3f01c9a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72531f2a35b469dbe795ab142dcf909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from huggingface\n"
     ]
    }
   ],
   "source": [
    "LOCAL_DIR = \"./ml/\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR + \"roberta/\", model_max_length=max_token_length)\n",
    "    print(\"Tokenizer loaded from local directory\")\n",
    "except:\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, model_max_length=max_token_length)\n",
    "    roberta_tokenizer.save_pretrained(LOCAL_DIR + \"roberta/\")\n",
    "    print(\"Tokenizer loaded from huggingface\")\n",
    "\n",
    "def roberta_tokenize_function(examples):\n",
    "    return roberta_tokenizer(examples[\"text\"], truncation=True)\n",
    "    \n",
    "roberta_tokenized_dataset = train_test.map(roberta_tokenize_function, batched=True)\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "roberta_full_collected_tokenized = collected_dataset.map(roberta_tokenize_function, batched=True)\n",
    "\n",
    "# Load model\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)\n",
    "print(\"Model loaded from huggingface\")\n",
    "\n",
    "def roberta_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06a265-7bd2-4356-9922-6fdffd4b2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_multilingual\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=roberta_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    model_init=roberta_init,\n",
    "    data_collator=roberta_data_collator,\n",
    ")\n",
    "\n",
    "best_roberta_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_roberta_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c9bcf61-5156-4e37-8575-d35905da36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ml/best_hparams/roberta.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "best_hparams\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_multilingual\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7710e2-ef28-4cdc-a99a-24eade511d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=roberta_tokenized_dataset[\"test\"],\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    data_collator=roberta_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "trainer.train()\n",
    "total_time_taken = (datetime.now() - start_time).total_seconds()\n",
    "print(\"Training took\", total_time_taken, \"seconds\")\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(LOCAL_DIR + \"trained_roberta_model/\")\n",
    "del(roberta_model)\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7107d2fb-ea9f-4dd1-bbd0-caedb46ac4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3780\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3780' max='3780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3780/3780 20:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.775400</td>\n",
       "      <td>1.757840</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.274821</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.274821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.774600</td>\n",
       "      <td>1.756709</td>\n",
       "      <td>0.075561</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.118483</td>\n",
       "      <td>0.274292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.768800</td>\n",
       "      <td>1.756278</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.274821</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.274821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.768000</td>\n",
       "      <td>1.754882</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.274821</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.274821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-945\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-1890\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-1890/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-2835\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-2835/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-2835/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-2835/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-2835/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-3780\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-3780/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-3780/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-3780/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-3780/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_multilingual/checkpoint-3780 (score: 1.754881501197815).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 01:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3780' max='3780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3780/3780 20:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.628800</td>\n",
       "      <td>1.758419</td>\n",
       "      <td>0.089273</td>\n",
       "      <td>0.274821</td>\n",
       "      <td>0.119166</td>\n",
       "      <td>0.274821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.645200</td>\n",
       "      <td>1.492491</td>\n",
       "      <td>0.195952</td>\n",
       "      <td>0.405083</td>\n",
       "      <td>0.255471</td>\n",
       "      <td>0.405083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.386500</td>\n",
       "      <td>0.998689</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.613185</td>\n",
       "      <td>0.524799</td>\n",
       "      <td>0.613185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.960900</td>\n",
       "      <td>0.937526</td>\n",
       "      <td>0.499253</td>\n",
       "      <td>0.624305</td>\n",
       "      <td>0.536451</td>\n",
       "      <td>0.624305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-945\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-1890\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-1890/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-2835\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-2835/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-2835/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-2835/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-2835/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-3780\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-3780/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-3780/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-3780/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-3780/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_multilingual/checkpoint-3780 (score: 0.9375255703926086).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 01:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3780' max='3780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3780/3780 20:54, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.628800</td>\n",
       "      <td>1.758419</td>\n",
       "      <td>0.089273</td>\n",
       "      <td>0.274821</td>\n",
       "      <td>0.119166</td>\n",
       "      <td>0.274821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.645200</td>\n",
       "      <td>1.492491</td>\n",
       "      <td>0.195952</td>\n",
       "      <td>0.405083</td>\n",
       "      <td>0.255471</td>\n",
       "      <td>0.405083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.386500</td>\n",
       "      <td>0.998689</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.613185</td>\n",
       "      <td>0.524799</td>\n",
       "      <td>0.613185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.960900</td>\n",
       "      <td>0.937526</td>\n",
       "      <td>0.499253</td>\n",
       "      <td>0.624305</td>\n",
       "      <td>0.536451</td>\n",
       "      <td>0.624305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-945\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-1890\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-1890/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-2835\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-2835/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-2835/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-2835/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-2835/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n",
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ml/results/roberta_multilingual/checkpoint-3780\n",
      "Configuration saved in ml/results/roberta_multilingual/checkpoint-3780/config.json\n",
      "Model weights saved in ml/results/roberta_multilingual/checkpoint-3780/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_multilingual/checkpoint-3780/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_multilingual/checkpoint-3780/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_multilingual/checkpoint-3780 (score: 0.9375255703926086).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 01:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRoberta Accuracy: 50.7810% +/- 16.4748 Weighted F1: 39.7130% +/- 19.7029\n",
      "XLMRoberta Transferability: 21.2855% +/- 2.7266 Weighted F1: 10.7164% +/- 3.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=roberta_tokenized_dataset[\"test\"],\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    roberta_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = roberta_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = roberta_results[\"eval_f1\"]\n",
    "    roberta_results = trainer.evaluate(eval_dataset=roberta_full_collected_tokenized)\n",
    "    transferability_acc[i] = roberta_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = roberta_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"XLMRoberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"XLMRoberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11cbc52-42e6-47f4-8044-c03e429cf06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
