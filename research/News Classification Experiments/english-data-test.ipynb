{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e7e73d-d1f2-4f39-9c3d-51640100670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /root/anaconda3/lib/python3.9/site-packages (4.23.1)\n",
      "Requirement already satisfied: datasets in /root/anaconda3/lib/python3.9/site-packages (2.7.1)\n",
      "Requirement already satisfied: evaluate in /root/anaconda3/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /root/anaconda3/lib/python3.9/site-packages (0.12.1)\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /root/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /root/anaconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/lib/python3.9/site-packages (from huggingface-hub) (4.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /root/anaconda3/lib/python3.9/site-packages (from optuna) (1.4.22)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/anaconda3/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /root/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/anaconda3/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/root/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets evaluate huggingface-hub optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4262fd-8c72-47f0-b511-6e3d41095787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-02-27 00:41:25.959619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 00:41:26.103906: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-27 00:41:26.606607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-27 00:41:26.606687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-27 00:41:26.606694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, create_optimizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from optuna import trial\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecc9991-e8f3-4bf0-89a9-10d5005c47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837fc1ef-e529-4bee-9207-5591dbf32eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a08e54a29f2475eadba76e33dcca600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de84489-3106-42d5-b2a0-c39d5e52fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b0d047-c466-4093-bc04-5537cd504bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3147\n",
      "})\n",
      "Tokenizer loaded from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614b765fb9244dfba60bd732fbbaf9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1c22e71c1646fab64e586024161a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from huggingface\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"bert-base-multilingual-cased\"\n",
    "BASE_DATASET = \"valurank/News_Articles_Categorization\"\n",
    "LOCAL_DIR = \"./ml/\"\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = load_from_disk(LOCAL_DIR + \"/datasets/public_en\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    # Combine science and tech category\n",
    "    label2id = {\"Politics\" : 2, \"Tech\": 3, \"Entertainment\": 0, \"Sports\": 1, \"Business\": 4, \"Health\" : 5, \"science\": 3}\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "\n",
    "    dataset = load_dataset(BASE_DATASET, split=\"train\")\n",
    "    dataset = dataset.rename_column(\"Text\", \"text\")\n",
    "    dataset = dataset.rename_column(\"Category\", \"label\")\n",
    "    # Drop \"World\" category\n",
    "    dataset = dataset.filter(lambda i: i[\"label\"] != \"World\")\n",
    "    \n",
    "    features = dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    dataset = dataset.map(label_to_id, batched=True, features=features)\n",
    "    \n",
    "    dataset.save_to_disk(LOCAL_DIR + \"/datasets/public_en\")\n",
    "    print(\"Loaded dataset from huggingface\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "# Split train and test\n",
    "train_test = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR + \"mbert/\", model_max_length=max_token_length)\n",
    "    print(\"Tokenizer loaded from local directory\")\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, model_max_length=max_token_length)\n",
    "    tokenizer.save_pretrained(LOCAL_DIR + \"mbert/\")\n",
    "    print(\"Tokenizer loaded from huggingface\")\n",
    "\n",
    "#From https://huggingface.co/docs/transformers/main/en/training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = train_test.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "print(\"Model loaded from huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3970ffa4-6a4e-4e41-8ffc-9aabd31ab155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ml/datasets/collected_multilingual/cache-e5ec5202f5b6d5fb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 12846\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1549ef37a4b144f890a076c5ba96085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ca732e589745b0a68ed2e530b2e38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process collected data\n",
    "# Load dataset\n",
    "try:\n",
    "    collected_dataset = load_from_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "    label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "    collected_dataset = load_dataset('json', data_files=LOCAL_DIR + \"datasets/collected.json\", split=\"train\")\n",
    "    features = collected_dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    collected_dataset = collected_dataset.map(label_to_id, batched=True, features=features)\n",
    "    collected_dataset = collected_dataset.shuffle(seed=42)\n",
    "    \n",
    "    collected_dataset.save_to_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded dataset from original file\")\n",
    "\n",
    "print(collected_dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "full_collected_tokenized = collected_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split train and test\n",
    "collected_train_test = collected_dataset.train_test_split(test_size=test_split)\n",
    "collected_tokenized_dataset = collected_train_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fae45a-865c-4a3a-b95c-795dcbfde5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "# From https://huggingface.co/spaces/BucketHeadP65/confusion_matrix\n",
    "cfm = evaluate.load(\"BucketHeadP65/confusion_matrix\")\n",
    "\n",
    "# From https://huggingface.co/docs/transformers/main/en/tasks/sequence_classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    # confusion_matrix = cfm.compute(predictions=predictions, references=labels)[\"confusion_matrix\"]\n",
    "\n",
    "    return {\"precision\": precision_score, \"recall\": recall_score, \"f1\": f1_score,\n",
    "     \"accuracy\": accuracy_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cab4ba-8daf-4aa3-97f9-42b44073b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://huggingface.co/docs/transformers/hpo_train\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 4e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4]),\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09921351-7452-4d04-8367-c851e10294bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-02-26 02:25:56,879]\u001b[0m A new study created in memory with name: no-name-ea0a65d6-492a-4f71-b729-f40c2fa04aa1\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 02:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304333</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-0/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-0/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-0/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-0/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-0/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-0/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-0/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-0/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-0/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-0/checkpoint-158/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:28:04,047]\u001b[0m Trial 0 finished with value: 3.8412698412698414 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 0 with value: 3.8412698412698414.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='632' max='632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [632/632 04:21, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.198188</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.141422</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131322</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.147443</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-1/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-1/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-1/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-1/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-1/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-1/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-1/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-1/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-1/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-1/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-1/checkpoint-474\n",
      "Configuration saved in ml/results/trained_eng_model/run-1/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-1/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-1/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-1/checkpoint-474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-1/checkpoint-632\n",
      "Configuration saved in ml/results/trained_eng_model/run-1/checkpoint-632/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-1/checkpoint-632/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-1/checkpoint-632/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-1/checkpoint-632/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:32:28,154]\u001b[0m Trial 1 finished with value: 3.8412698412698414 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}. Best is trial 0 with value: 3.8412698412698414.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 03:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140629</td>\n",
       "      <td>0.953968</td>\n",
       "      <td>0.953968</td>\n",
       "      <td>0.953968</td>\n",
       "      <td>0.953968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119910</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>0.958730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-2/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-2/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-2/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-2/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-2/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-2/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-2/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-2/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-2/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-2/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-2/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-2/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-2/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-2/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-2/checkpoint-237/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:35:35,120]\u001b[0m Trial 2 finished with value: 3.834920634920635 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 0 with value: 3.8412698412698414.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 02:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.263332</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.934921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.170359</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-3/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-3/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-3/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-3/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-3/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-3/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-3/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-3/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-3/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-3/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:37:47,057]\u001b[0m Trial 3 finished with value: 3.8095238095238093 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 0 with value: 3.8412698412698414.\u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249691</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137644</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.159113</td>\n",
       "      <td>0.963492</td>\n",
       "      <td>0.963492</td>\n",
       "      <td>0.963492</td>\n",
       "      <td>0.963492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-4/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-4/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-4/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-4/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-4/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-4/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-4/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-4/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-4/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-4/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-4/checkpoint-474\n",
      "Configuration saved in ml/results/trained_eng_model/run-4/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-4/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-4/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-4/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:41:07,109]\u001b[0m Trial 4 finished with value: 3.853968253968254 and parameters: {'learning_rate': 4e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 4 with value: 3.853968253968254.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-5/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-5/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-5/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-5/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-5/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-5/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-5/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-5/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-5/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-5/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-5/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-5/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-5/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-5/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-5/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-5/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-5/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-5/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-5/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-5/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:45:18,926]\u001b[0m Trial 5 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.281682</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114250</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.968254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-6/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-6/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-6/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-6/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-6/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-6/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-6/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-6/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-6/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-6/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-6/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-6/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-6/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-6/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-6/checkpoint-237/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:48:29,707]\u001b[0m Trial 6 finished with value: 3.873015873015873 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 02:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.280302</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.926984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.147310</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-7/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-7/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-7/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-7/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-7/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-7/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-7/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-7/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-7/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-7/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 02:51:16,998]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/474 00:43 < 01:28, 3.56 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249691</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.931746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 02:52:02,939]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='632' max='632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [632/632 04:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.198188</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.141422</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131322</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.147443</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-9/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-9/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-9/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-9/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-9/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-9/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-9/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-9/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-9/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-9/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-9/checkpoint-474\n",
      "Configuration saved in ml/results/trained_eng_model/run-9/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-9/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-9/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-9/checkpoint-474/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-9/checkpoint-632\n",
      "Configuration saved in ml/results/trained_eng_model/run-9/checkpoint-632/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-9/checkpoint-632/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-9/checkpoint-632/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-9/checkpoint-632/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 02:56:27,986]\u001b[0m Trial 9 finished with value: 3.8412698412698414 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-10/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-10/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-10/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-10/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-10/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-10/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-10/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-10/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-10/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-10/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-10/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-10/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-10/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-10/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-10/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-10/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-10/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-10/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-10/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-10/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:00:40,206]\u001b[0m Trial 10 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-11/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-11/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-11/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-11/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-11/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-11/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-11/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-11/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-11/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-11/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-11/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-11/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-11/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-11/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-11/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-11/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-11/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-11/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-11/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-11/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:04:52,835]\u001b[0m Trial 11 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-12/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-12/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-12/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-12/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-12/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-12/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-12/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-12/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-12/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-12/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-12/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-12/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-12/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-12/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-12/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-12/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-12/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-12/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-12/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-12/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:09:04,488]\u001b[0m Trial 12 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-13/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-13/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-13/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-13/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-13/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-13/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-13/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-13/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-13/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-13/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-13/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-13/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-13/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-13/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-13/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-13/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-13/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-13/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-13/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-13/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:13:17,601]\u001b[0m Trial 13 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-14/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-14/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-14/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-14/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-14/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-14/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-14/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-14/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-14/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-14/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-14/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-14/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-14/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-14/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-14/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-14/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-14/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-14/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-14/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-14/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:17:29,416]\u001b[0m Trial 14 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/316 00:40 < 02:04, 1.91 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277147</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 03:18:12,062]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-16/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-16/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-16/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-16/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-16/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-16/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-16/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-16/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-16/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-16/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-16/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-16/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-16/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-16/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-16/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-16/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-16/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-16/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-16/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-16/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:22:22,568]\u001b[0m Trial 16 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/158 00:40 < 00:41, 1.91 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304333</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 03:23:05,155]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-18/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-18/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-18/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-18/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-18/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-18/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-18/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-18/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-18/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-18/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-18/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-18/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-18/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-18/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-18/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-18/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-18/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-18/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-18/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-18/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:27:17,269]\u001b[0m Trial 18 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/316 00:40 < 02:04, 1.91 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277147</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.920635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 03:28:00,321]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/158 00:40 < 00:41, 1.91 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304333</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.907937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 03:28:43,276]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-21/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-21/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-21/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-21/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-21/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-21/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-21/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-21/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-21/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-21/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-21/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-21/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-21/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-21/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-21/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-21/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-21/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-21/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-21/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-21/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:32:53,893]\u001b[0m Trial 21 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-22/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-22/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-22/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-22/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-22/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-22/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-22/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-22/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-22/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-22/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-22/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-22/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-22/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-22/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-22/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-22/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-22/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-22/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-22/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-22/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:37:03,511]\u001b[0m Trial 22 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248675</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184361</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.950794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114225</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.973016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.974603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-23/checkpoint-79\n",
      "Configuration saved in ml/results/trained_eng_model/run-23/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-23/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-23/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-23/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-23/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/run-23/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-23/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-23/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-23/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-23/checkpoint-237\n",
      "Configuration saved in ml/results/trained_eng_model/run-23/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-23/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-23/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-23/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_eng_model/run-23/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/run-23/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/run-23/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/run-23/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/run-23/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 03:41:22,571]\u001b[0m Trial 23 finished with value: 3.8984126984126983 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 5 with value: 3.8984126984126983.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_eng_model\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/mbert-english.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8467e2d7-bb57-444a-8759-0551c8addb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ml/best_hparams/mbert-english.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/mbert-english\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d98bf8-a5af-4be2-98e7-d991d31ce486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.241956</td>\n",
       "      <td>0.937622</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>0.930561</td>\n",
       "      <td>0.930159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.175870</td>\n",
       "      <td>0.952695</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952438</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.158301</td>\n",
       "      <td>0.964085</td>\n",
       "      <td>0.963492</td>\n",
       "      <td>0.963679</td>\n",
       "      <td>0.963492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.122534</td>\n",
       "      <td>0.962673</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-79\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-158\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-237\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-316\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-english/checkpoint-316 (score: 0.12253432720899582).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317401</td>\n",
       "      <td>0.908924</td>\n",
       "      <td>0.903175</td>\n",
       "      <td>0.903428</td>\n",
       "      <td>0.903175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133073</td>\n",
       "      <td>0.964145</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.962256</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.967541</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966763</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121459</td>\n",
       "      <td>0.972955</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.971613</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-79\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-158\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-237\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-316\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-english/checkpoint-237 (score: 0.10566705465316772).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 04:14, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317401</td>\n",
       "      <td>0.908924</td>\n",
       "      <td>0.903175</td>\n",
       "      <td>0.903428</td>\n",
       "      <td>0.903175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133073</td>\n",
       "      <td>0.964145</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.962256</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.967541</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966763</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121459</td>\n",
       "      <td>0.972955</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.971613</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-79\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-158\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-237\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-237/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ml/results/mbert-english/checkpoint-316\n",
      "Configuration saved in ml/results/mbert-english/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/mbert-english/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-english/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-english/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-english/checkpoint-237 (score: 0.10566705465316772).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBert Accuracy: 96.5079% +/- 0.2245 Weighted F1: 96.5239% +/- 0.2155\n",
      "MBert Transferability: 65.7792% +/- 0.2092 Weighted F1: 65.9601% +/- 0.1353\n"
     ]
    }
   ],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "    mbert_results = trainer.evaluate(eval_dataset=full_collected_tokenized)\n",
    "    transferability_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"MBert Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"MBert Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/mbert-eng\", \"w\") as f:\n",
    "    f.write(f\"MBert Eng Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")\n",
    "    f.write(f\"MBert Eng Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0018512e-0f7c-4657-ad65-5b58a08e149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ml/trained_eng_model/\n",
      "Configuration saved in ./ml/trained_eng_model/config.json\n",
      "Model weights saved in ./ml/trained_eng_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./ml/trained_eng_model/tokenizer_config.json\n",
      "Special tokens file saved in ./ml/trained_eng_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(LOCAL_DIR + \"trained_eng_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949f5b22-f8f0-4d4b-9d4b-667b1fd20baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6a7e4005e94ca6a6c8454e5f997913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba210dbad9c4e88a73ffe9ea1773d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ml/datasets/collected_multilingual/cache-e4cff256ac0a723f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a3f05d21364a6b9385874231a9b3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e904e706a114549b5f8769c60d16e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from huggingface\n"
     ]
    }
   ],
   "source": [
    "LOCAL_DIR = \"./ml/\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR + \"roberta/\", model_max_length=max_token_length)\n",
    "    print(\"Tokenizer loaded from local directory\")\n",
    "except:\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, model_max_length=max_token_length)\n",
    "    roberta_tokenizer.save_pretrained(LOCAL_DIR + \"roberta/\")\n",
    "    print(\"Tokenizer loaded from huggingface\")\n",
    "\n",
    "def roberta_tokenize_function(examples):\n",
    "    return roberta_tokenizer(examples[\"text\"], truncation=True)\n",
    "    \n",
    "roberta_tokenized_dataset = train_test.map(roberta_tokenize_function, batched=True)\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "roberta_full_collected_tokenized = collected_dataset.map(roberta_tokenize_function, batched=True) \n",
    "roberta_collected_tokenized_dataset = collected_train_test.map(roberta_tokenize_function, batched=True)\n",
    "# Load model\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)\n",
    "print(\"Model loaded from huggingface\")\n",
    "\n",
    "def roberta_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0995a20e-94aa-44ff-ba2d-289a821b2b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-02-26 03:58:02,333]\u001b[0m A new study created in memory with name: no-name-38251e54-ed3a-49c1-98fb-13773632cef8\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-0/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-0/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-0/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-0/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-0/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-0/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-0/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-0/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-0/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-0/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-0/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-0/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-0/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-0/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-0/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:02:06,265]\u001b[0m Trial 0 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 02:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.483792</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.425164</td>\n",
       "      <td>0.398413</td>\n",
       "      <td>0.398413</td>\n",
       "      <td>0.398413</td>\n",
       "      <td>0.398413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-1/checkpoint-79\n",
      "Configuration saved in ml/results/roberta_eng/run-1/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-1/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-1/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-1/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-1/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-1/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-1/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-1/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-1/checkpoint-158/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:04:40,177]\u001b[0m Trial 1 finished with value: 1.5936507936507935 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.451386</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.337377</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.245697</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-2/checkpoint-79\n",
      "Configuration saved in ml/results/roberta_eng/run-2/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-2/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-2/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-2/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-2/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-2/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-2/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-2/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-2/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-2/checkpoint-237\n",
      "Configuration saved in ml/results/roberta_eng/run-2/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-2/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-2/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-2/checkpoint-237/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:08:30,539]\u001b[0m Trial 2 finished with value: 1.892063492063492 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 02:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.410173</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.406349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.323046</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-3/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-3/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-3/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-3/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-3/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-3/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-3/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-3/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-3/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-3/checkpoint-316/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:11:12,862]\u001b[0m Trial 3 finished with value: 1.7777777777777777 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 02:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.548220</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.478224</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.403175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-4/checkpoint-79\n",
      "Configuration saved in ml/results/roberta_eng/run-4/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-4/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-4/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-4/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-4/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-4/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-4/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-4/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-4/checkpoint-158/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:13:45,581]\u001b[0m Trial 4 finished with value: 1.6126984126984127 and parameters: {'learning_rate': 4e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/158 00:41 < 00:42, 1.86 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.483792</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:14:30,008]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/237 00:41 < 01:25, 1.85 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.497135</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:15:14,599]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/316 00:41 < 02:08, 1.85 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.477296</td>\n",
       "      <td>0.401587</td>\n",
       "      <td>0.401587</td>\n",
       "      <td>0.401587</td>\n",
       "      <td>0.401587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:15:59,159]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.429385</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.300055</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.460317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.260187</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.465079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-8/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-8/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-8/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-8/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-8/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-8/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-8/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-8/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-8/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-8/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:19:27,343]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.451386</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.407937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.337377</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.245697</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "      <td>0.473016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-9/checkpoint-79\n",
      "Configuration saved in ml/results/roberta_eng/run-9/checkpoint-79/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-9/checkpoint-79/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-9/checkpoint-79/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-9/checkpoint-79/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-9/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-9/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-9/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-9/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-9/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-9/checkpoint-237\n",
      "Configuration saved in ml/results/roberta_eng/run-9/checkpoint-237/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-9/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-9/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-9/checkpoint-237/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:23:19,215]\u001b[0m Trial 9 finished with value: 1.892063492063492 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/632 00:46 < 02:20, 3.38 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.643679</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:24:08,232]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-11/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-11/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-11/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-11/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-11/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-11/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-11/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-11/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-11/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-11/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-11/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-11/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-11/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-11/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-11/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:28:09,571]\u001b[0m Trial 11 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-12/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-12/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-12/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-12/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-12/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-12/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-12/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-12/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-12/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-12/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-12/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-12/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-12/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-12/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-12/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:32:11,177]\u001b[0m Trial 12 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-13/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-13/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-13/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-13/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-13/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-13/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-13/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-13/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-13/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-13/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-13/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-13/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-13/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-13/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-13/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:36:13,989]\u001b[0m Trial 13 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-14/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-14/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-14/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-14/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-14/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-14/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-14/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-14/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-14/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-14/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-14/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-14/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-14/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-14/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-14/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:40:15,878]\u001b[0m Trial 14 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-15/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-15/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-15/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-15/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-15/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-15/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-15/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-15/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-15/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-15/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-15/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-15/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-15/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-15/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-15/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:44:16,833]\u001b[0m Trial 15 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/632 00:46 < 02:20, 3.38 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.643679</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.360317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:45:06,614]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-17/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-17/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-17/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-17/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-17/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-17/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-17/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-17/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-17/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-17/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-17/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-17/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-17/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-17/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-17/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:49:09,248]\u001b[0m Trial 17 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-18/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-18/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-18/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-18/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-18/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-18/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-18/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-18/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-18/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-18/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-18/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-18/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-18/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-18/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-18/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:53:14,035]\u001b[0m Trial 18 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 4e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/632 00:46 < 02:20, 3.38 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.471930</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2023-02-26 04:54:03,549]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-20/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-20/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-20/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-20/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-20/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-20/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-20/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-20/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-20/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-20/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-20/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-20/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-20/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-20/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-20/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 04:58:05,749]\u001b[0m Trial 20 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-21/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-21/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-21/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-21/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-21/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-21/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-21/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-21/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-21/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-21/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-21/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-21/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-21/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-21/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-21/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 05:02:10,707]\u001b[0m Trial 21 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-22/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-22/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-22/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-22/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-22/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-22/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-22/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-22/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-22/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-22/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-22/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-22/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-22/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-22/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-22/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 05:06:12,997]\u001b[0m Trial 22 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.438403</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279785</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.108483</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.520635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-23/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/run-23/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-23/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-23/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-23/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-23/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/run-23/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-23/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-23/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-23/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/roberta_eng/run-23/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/run-23/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/run-23/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/run-23/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/run-23/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-26 05:10:16,439]\u001b[0m Trial 23 finished with value: 2.0825396825396827 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 2.0825396825396827.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_eng\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=roberta_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    model_init=roberta_init,\n",
    "    data_collator=roberta_data_collator,\n",
    ")\n",
    "\n",
    "best_roberta_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta-eng.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_roberta_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84679384-f8e8-4295-aec3-633733814265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ml/best_hparams/roberta-eng.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_eng\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12fd5f53-8e5e-40e1-9ae7-b99ef2e9711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.764493</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.625168</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.360603</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_eng/checkpoint-474 (score: 1.3606027364730835).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='843' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.763617</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.762103</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.760905</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_eng/checkpoint-474 (score: 1.760905385017395).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='843' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 04:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.763617</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.762103</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.760905</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "      <td>0.265079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-158\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-316\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_eng/checkpoint-474\n",
      "Configuration saved in ml/results/roberta_eng/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/roberta_eng/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_eng/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_eng/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_eng/checkpoint-474 (score: 1.760905385017395).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='843' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12846\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRoberta Accuracy: 31.3228% +/- 6.8092 Weighted F1: 31.3228% +/- 6.8092\n",
      "XLMRoberta Transferability: 17.8343% +/- 0.5725 Weighted F1: 17.8343% +/- 0.5725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ml/trained_eng_roberta_model/\n",
      "Configuration saved in ./ml/trained_eng_roberta_model/config.json\n",
      "Model weights saved in ./ml/trained_eng_roberta_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./ml/trained_eng_roberta_model/tokenizer_config.json\n",
      "Special tokens file saved in ./ml/trained_eng_roberta_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=roberta_tokenized_dataset[\"test\"],\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    roberta_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = roberta_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = roberta_results[\"eval_f1\"]\n",
    "    roberta_results = trainer.evaluate(eval_dataset=roberta_full_collected_tokenized)\n",
    "    transferability_acc[i] = roberta_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = roberta_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"XLMRoberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"XLMRoberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/roberta-eng\", \"w\") as f:\n",
    "    f.write(f\"XLMRoberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")\n",
    "    f.write(f\"XLMRoberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")\n",
    "    \n",
    "trainer.save_model(LOCAL_DIR + \"trained_eng_roberta_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a97577-f325-4c03-8d5e-59c7c0670760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2517\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 03:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.251920</td>\n",
       "      <td>0.947619</td>\n",
       "      <td>0.947619</td>\n",
       "      <td>0.947619</td>\n",
       "      <td>0.947619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186186</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.207080</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/trained_eng_model/checkpoint-158\n",
      "Configuration saved in ml/results/trained_eng_model/checkpoint-158/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/checkpoint-158/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/trained_eng_model/checkpoint-316\n",
      "Configuration saved in ml/results/trained_eng_model/checkpoint-316/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/checkpoint-316/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/trained_eng_model/checkpoint-474\n",
      "Configuration saved in ml/results/trained_eng_model/checkpoint-474/config.json\n",
      "Model weights saved in ml/results/trained_eng_model/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_eng_model/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_eng_model/checkpoint-474/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/trained_eng_model/checkpoint-316 (score: 0.18618646264076233).\n",
      "Saving model checkpoint to ./ml/trained_eng_model/\n",
      "Configuration saved in ./ml/trained_eng_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 204.786592 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./ml/trained_eng_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./ml/trained_eng_model/tokenizer_config.json\n",
      "Special tokens file saved in ./ml/trained_eng_model/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'private'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12840/1707574244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"trained_eng_model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3407\u001b[0m         \u001b[0;31m# it might fail.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3262\u001b[0;31m             self.repo = Repository(\n\u001b[0m\u001b[1;32m   3263\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'private'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_eng_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "trainer.train()\n",
    "total_time_taken = (datetime.now() - start_time).total_seconds()\n",
    "print(\"Training took\", total_time_taken, \"seconds\")\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(LOCAL_DIR + \"trained_eng_model/\")\n",
    "trainer.push_to_hub()\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107d2fb-ea9f-4dd1-bbd0-caedb46ac4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-02-26 14:24:08,799]\u001b[0m A new study created in memory with name: no-name-3eafdc99-a24d-4d91-b324-49649d0947c9\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10276\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 966\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='577' max='966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [577/966 05:06 < 03:27, 1.88 it/s, Epoch 1.79/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.384481</td>\n",
       "      <td>0.869650</td>\n",
       "      <td>0.869650</td>\n",
       "      <td>0.869650</td>\n",
       "      <td>0.869650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/mbert_realworld_model/run-0/checkpoint-322\n",
      "Configuration saved in ml/results/mbert_realworld_model/run-0/checkpoint-322/config.json\n",
      "Model weights saved in ml/results/mbert_realworld_model/run-0/checkpoint-322/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert_realworld_model/run-0/checkpoint-322/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert_realworld_model/run-0/checkpoint-322/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/mbert_realworld_model\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=collected_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=collected_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/mbert-realworld.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d7c0d8-8d00-4440-b071-b0c76dba15be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ml/best_hparams/mbert-realworld.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/mbert-realworld\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b779f6-ee39-49dd-b8f3-0473caf9af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10276\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1929' max='1929' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1929/1929 10:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.670600</td>\n",
       "      <td>0.467182</td>\n",
       "      <td>0.868179</td>\n",
       "      <td>0.867704</td>\n",
       "      <td>0.866994</td>\n",
       "      <td>0.867704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.390083</td>\n",
       "      <td>0.880127</td>\n",
       "      <td>0.874708</td>\n",
       "      <td>0.873910</td>\n",
       "      <td>0.874708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.422206</td>\n",
       "      <td>0.891736</td>\n",
       "      <td>0.891829</td>\n",
       "      <td>0.891611</td>\n",
       "      <td>0.891829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-643\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-643/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-643/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-643/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-643/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1286\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1286/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1286/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1286/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1286/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1929\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1929/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1929/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1929/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1929/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-realworld/checkpoint-1286 (score: 0.39008328318595886).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10276\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1929' max='1929' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1929/1929 10:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.445837</td>\n",
       "      <td>0.860761</td>\n",
       "      <td>0.856031</td>\n",
       "      <td>0.856018</td>\n",
       "      <td>0.856031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.395511</td>\n",
       "      <td>0.884769</td>\n",
       "      <td>0.879767</td>\n",
       "      <td>0.879358</td>\n",
       "      <td>0.879767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.423185</td>\n",
       "      <td>0.895361</td>\n",
       "      <td>0.895331</td>\n",
       "      <td>0.895233</td>\n",
       "      <td>0.895331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-643\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-643/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-643/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-643/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-643/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1286\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1286/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1286/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1286/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1286/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1929\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1929/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1929/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1929/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1929/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-realworld/checkpoint-1286 (score: 0.395510733127594).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10276\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1929' max='1929' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1929/1929 10:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.445837</td>\n",
       "      <td>0.860761</td>\n",
       "      <td>0.856031</td>\n",
       "      <td>0.856018</td>\n",
       "      <td>0.856031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.395511</td>\n",
       "      <td>0.884769</td>\n",
       "      <td>0.879767</td>\n",
       "      <td>0.879358</td>\n",
       "      <td>0.879767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.423185</td>\n",
       "      <td>0.895361</td>\n",
       "      <td>0.895331</td>\n",
       "      <td>0.895233</td>\n",
       "      <td>0.895331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-643\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-643/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-643/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-643/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-643/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1286\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1286/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1286/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1286/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1286/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/mbert-realworld/checkpoint-1929\n",
      "Configuration saved in ml/results/mbert-realworld/checkpoint-1929/config.json\n",
      "Model weights saved in ml/results/mbert-realworld/checkpoint-1929/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/mbert-realworld/checkpoint-1929/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/mbert-realworld/checkpoint-1929/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/mbert-realworld/checkpoint-1286 (score: 0.395510733127594).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBert Accuracy: 87.8080% +/- 0.2385 Weighted F1: 87.7542% +/- 0.2569\n"
     ]
    }
   ],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=collected_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=collected_tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "\n",
    "print(f\"MBert Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/mbert-realworld\", \"w\") as f:\n",
    "    f.write(f\"MBert Realworld Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d80de-4599-4db1-a254-5112dde50a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_realworld\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_collected_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=roberta_collected_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    model_init=roberta_init,\n",
    "    data_collator=roberta_data_collator,\n",
    ")\n",
    "\n",
    "best_roberta_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta-realworld.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_roberta_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05acea24-2816-4857-ab5a-fb34b1535433",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ml/best_hparams/roberta-realworld.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_realworld\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ded62-43c5-4768-b169-adccc9cedc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_collected_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=roberta_collected_tokenized_dataset[\"test\"],\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    roberta_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = roberta_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = roberta_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "\n",
    "print(f\"XLMRoberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/roberta-realworld\", \"w\") as f:\n",
    "    f.write(f\"XLMRoberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")\n",
    "    \n",
    "trainer.save_model(LOCAL_DIR + \"trained_realworld_roberta_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a714206-4a20-4fc3-a941-00ad69d8e9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
