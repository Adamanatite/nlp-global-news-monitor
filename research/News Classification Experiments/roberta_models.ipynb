{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4262fd-8c72-47f0-b511-6e3d41095787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-02-28 20:23:56.776600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 20:23:56.965609: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-28 20:23:57.641284: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 20:23:57.641363: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 20:23:57.641370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification, DataCollatorWithPadding, create_optimizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from optuna import trial\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecc9991-e8f3-4bf0-89a9-10d5005c47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b0d047-c466-4093-bc04-5537cd504bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18882\n",
      "})\n",
      "Tokenizer loaded from huggingface\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c54d74bf234142acdc936fe9af9a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb1fe835ada41afb672e5a82522c942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713fbace73f94f8f803094b9ecd8a822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ea5af54bdc475db49a6d6a932fe8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from huggingface\n"
     ]
    }
   ],
   "source": [
    "max_token_length = 512\n",
    "BASE_MODEL = \"xlm-roberta-base\"\n",
    "BASE_DATASET = \"valurank/News_Articles_Categorization\"\n",
    "LOCAL_DIR = \"./ml/\"\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = load_from_disk(LOCAL_DIR + \"/datasets/public_multilingual\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    # Combine science and tech category\n",
    "    label2id = {\"Politics\" : 2, \"Tech\": 3, \"Entertainment\": 0, \"Sports\": 1, \"Business\": 4, \"Health\" : 5, \"science\": 3}\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "\n",
    "    dataset = load_dataset('json', data_files=LOCAL_DIR + \"datasets/ml.json\", split=\"train\")\n",
    "    # Drop \"World\" category\n",
    "    dataset = dataset.filter(lambda i: i[\"label\"] != \"World\")\n",
    "    \n",
    "    features = dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    dataset = dataset.map(label_to_id, batched=True, features=features)\n",
    "    \n",
    "    dataset.save_to_disk(LOCAL_DIR + \"/datasets/public_multilingual\")\n",
    "    print(\"Loaded dataset from original file\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "# Split train and test\n",
    "train_test = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "eng_dataset = load_from_disk(LOCAL_DIR + \"/datasets/public_en\")\n",
    "# Split train and test\n",
    "eng_train_test = eng_dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "print(\"Tokenizer loaded from huggingface\")\n",
    "\n",
    "#From https://huggingface.co/docs/transformers/main/en/training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = train_test.map(tokenize_function, batched=True)\n",
    "eng_tokenized_dataset = eng_train_test.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "# Load model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)\n",
    "print(\"Model loaded from huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974db2dc-5192-46f5-bdd2-2746f0cb2d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ml/datasets/collected_multilingual/cache-6ea2aa911ddd93d8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local dataset\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 12846\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688e052020c1436da59c53857ffa717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc1bb419ede4918b84be27bf4d13d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process collected data\n",
    "# Load dataset\n",
    "try:\n",
    "    collected_dataset = load_from_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded local dataset\")\n",
    "except Exception as e:\n",
    "    # Adapted from https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650\n",
    "    def label_to_id(batch):\n",
    "        batch[\"label\"] = [label2id[cat] for cat in batch[\"label\"]]\n",
    "        return batch\n",
    "    label2id = {\"Entertainment/Arts\": 0, \"Sports\": 1, \"Politics\": 2, \"Science/Technology\": 3, \"Business/Finance\": 4, \"Health/Welfare\": 5}\n",
    "    collected_dataset = load_dataset('json', data_files=LOCAL_DIR + \"datasets/collected.json\", split=\"train\")\n",
    "    features = collected_dataset.features.copy()\n",
    "    text_category = ClassLabel(num_classes = 6, names=[\"Entertainment/Arts\", \"Sports\", \"Politics\", \"Science/Technology\", \"Business/Finance\", \"Health/Welfare\"])\n",
    "    features[\"label\"] = text_category\n",
    "    collected_dataset = collected_dataset.map(label_to_id, batched=True, features=features)\n",
    "    collected_dataset = collected_dataset.shuffle(seed=42)\n",
    "    \n",
    "    collected_dataset.save_to_disk(LOCAL_DIR + \"/datasets/collected_multilingual\")\n",
    "    print(\"Loaded dataset from original file\")\n",
    "\n",
    "print(collected_dataset)\n",
    "\n",
    "# Split dataset based on fractions\n",
    "test_split = 0.2\n",
    "\n",
    "full_collected_tokenized = collected_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split train and test\n",
    "collected_train_test = collected_dataset.train_test_split(test_size=test_split)\n",
    "collected_tokenized_dataset = collected_train_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fae45a-865c-4a3a-b95c-795dcbfde5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "# From https://huggingface.co/spaces/BucketHeadP65/confusion_matrix\n",
    "cfm = evaluate.load(\"BucketHeadP65/confusion_matrix\")\n",
    "\n",
    "# From https://huggingface.co/docs/transformers/main/en/tasks/sequence_classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    #confusion_matrix = cfm.compute(predictions=predictions, references=labels)[\"confusion_matrix\"]\n",
    "\n",
    "    return {\"precision\": precision_score, \"recall\": recall_score, \"f1\": f1_score,\n",
    "     \"accuracy\": accuracy_score\n",
    "            #,\"confusion_matrix\": confusion_matrix\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cab4ba-8daf-4aa3-97f9-42b44073b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://huggingface.co/docs/transformers/hpo_train\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 4e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4]),\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    return XLMRobertaForSequenceClassification.from_pretrained(\n",
    "'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09921351-7452-4d04-8367-c851e10294bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-02-27 01:46:50,895]\u001b[0m A new study created in memory with name: no-name-9bf79538-4788-4274-a3d2-2dbbe68bfde3\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1892\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1892' max='1892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1892/1892 18:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146745</td>\n",
       "      <td>0.959731</td>\n",
       "      <td>0.958962</td>\n",
       "      <td>0.958861</td>\n",
       "      <td>0.958962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.066526</td>\n",
       "      <td>0.982597</td>\n",
       "      <td>0.982261</td>\n",
       "      <td>0.982347</td>\n",
       "      <td>0.982261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.054702</td>\n",
       "      <td>0.987679</td>\n",
       "      <td>0.987556</td>\n",
       "      <td>0.987581</td>\n",
       "      <td>0.987556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.060172</td>\n",
       "      <td>0.988567</td>\n",
       "      <td>0.988351</td>\n",
       "      <td>0.988386</td>\n",
       "      <td>0.988351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-0/checkpoint-473\n",
      "Configuration saved in ml/results/trained_roberta_model/run-0/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-0/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-0/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-0/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-0/checkpoint-946\n",
      "Configuration saved in ml/results/trained_roberta_model/run-0/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-0/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-0/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-0/checkpoint-946/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-0/checkpoint-1419\n",
      "Configuration saved in ml/results/trained_roberta_model/run-0/checkpoint-1419/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-0/checkpoint-1419/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-0/checkpoint-1419/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-0/checkpoint-1419/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-0/checkpoint-1892\n",
      "Configuration saved in ml/results/trained_roberta_model/run-0/checkpoint-1892/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-0/checkpoint-1892/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-0/checkpoint-1892/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-0/checkpoint-1892/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-27 02:05:47,929]\u001b[0m Trial 0 finished with value: 3.953654082484459 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4}. Best is trial 0 with value: 3.953654082484459.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1890' max='1890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1890/1890 10:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.130302</td>\n",
       "      <td>0.967873</td>\n",
       "      <td>0.966905</td>\n",
       "      <td>0.967049</td>\n",
       "      <td>0.966905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.084070</td>\n",
       "      <td>0.980649</td>\n",
       "      <td>0.980408</td>\n",
       "      <td>0.980428</td>\n",
       "      <td>0.980408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-1/checkpoint-945\n",
      "Configuration saved in ml/results/trained_roberta_model/run-1/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-1/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-1/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-1/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-1/checkpoint-1890\n",
      "Configuration saved in ml/results/trained_roberta_model/run-1/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-1/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-1/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-1/checkpoint-1890/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-27 02:16:14,062]\u001b[0m Trial 1 finished with value: 3.921891698538341 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 0 with value: 3.953654082484459.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2835\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2835' max='2835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2835/2835 15:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>0.956658</td>\n",
       "      <td>0.954991</td>\n",
       "      <td>0.954819</td>\n",
       "      <td>0.954991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.063368</td>\n",
       "      <td>0.983132</td>\n",
       "      <td>0.983055</td>\n",
       "      <td>0.983073</td>\n",
       "      <td>0.983055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.053887</td>\n",
       "      <td>0.987887</td>\n",
       "      <td>0.987821</td>\n",
       "      <td>0.987828</td>\n",
       "      <td>0.987821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-2/checkpoint-945\n",
      "Configuration saved in ml/results/trained_roberta_model/run-2/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-2/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-2/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-2/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-2/checkpoint-1890\n",
      "Configuration saved in ml/results/trained_roberta_model/run-2/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-2/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-2/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-2/checkpoint-1890/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-2/checkpoint-2835\n",
      "Configuration saved in ml/results/trained_roberta_model/run-2/checkpoint-2835/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-2/checkpoint-2835/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-2/checkpoint-2835/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-2/checkpoint-2835/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-27 02:31:46,978]\u001b[0m Trial 2 finished with value: 3.951357018959481 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 3.953654082484459.\u001b[0m\n",
      "Trial: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='946' max='946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [946/946 09:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.108015</td>\n",
       "      <td>0.966476</td>\n",
       "      <td>0.966375</td>\n",
       "      <td>0.966325</td>\n",
       "      <td>0.966375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.075217</td>\n",
       "      <td>0.981345</td>\n",
       "      <td>0.981202</td>\n",
       "      <td>0.981218</td>\n",
       "      <td>0.981202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-3/checkpoint-473\n",
      "Configuration saved in ml/results/trained_roberta_model/run-3/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-3/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-3/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-3/checkpoint-473/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-3/checkpoint-946\n",
      "Configuration saved in ml/results/trained_roberta_model/run-3/checkpoint-946/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-3/checkpoint-946/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-3/checkpoint-946/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-3/checkpoint-946/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-27 02:41:16,810]\u001b[0m Trial 3 finished with value: 3.9249671016281105 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 2}. Best is trial 0 with value: 3.953654082484459.\u001b[0m\n",
      "Trial: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1890' max='1890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1890/1890 10:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.183492</td>\n",
       "      <td>0.950751</td>\n",
       "      <td>0.948636</td>\n",
       "      <td>0.948599</td>\n",
       "      <td>0.948636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.103594</td>\n",
       "      <td>0.973158</td>\n",
       "      <td>0.972994</td>\n",
       "      <td>0.973028</td>\n",
       "      <td>0.972994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-4/checkpoint-945\n",
      "Configuration saved in ml/results/trained_roberta_model/run-4/checkpoint-945/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-4/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-4/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-4/checkpoint-945/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-4/checkpoint-1890\n",
      "Configuration saved in ml/results/trained_roberta_model/run-4/checkpoint-1890/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-4/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-4/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-4/checkpoint-1890/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-27 02:51:40,529]\u001b[0m Trial 4 finished with value: 3.8921748288854157 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 0 with value: 3.953654082484459.\u001b[0m\n",
      "Trial: {'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Entertainment/Arts\",\n",
      "    \"1\": \"Sports\",\n",
      "    \"2\": \"Politics\",\n",
      "    \"3\": \"Science/Technology\",\n",
      "    \"4\": \"Business/Finance\",\n",
      "    \"5\": \"Health/Welfare\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Business/Finance\": 4,\n",
      "    \"Entertainment/Arts\": 0,\n",
      "    \"Health/Welfare\": 5,\n",
      "    \"Politics\": 2,\n",
      "    \"Science/Technology\": 3,\n",
      "    \"Sports\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15105\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='673' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 673/1419 06:19 < 07:02, 1.77 it/s, Epoch 1.42/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105930</td>\n",
       "      <td>0.966890</td>\n",
       "      <td>0.966640</td>\n",
       "      <td>0.966631</td>\n",
       "      <td>0.966640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3777\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ml/results/trained_roberta_model/run-5/checkpoint-473\n",
      "Configuration saved in ml/results/trained_roberta_model/run-5/checkpoint-473/config.json\n",
      "Model weights saved in ml/results/trained_roberta_model/run-5/checkpoint-473/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/trained_roberta_model/run-5/checkpoint-473/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/trained_roberta_model/run-5/checkpoint-473/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_roberta_model\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5c1ab-314e-43ce-83ec-cffce4353ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ml/best_hparams/roberta.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_multilingual\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "best_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf499103-c5ee-45e5-b57c-81d6c99c48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "    mbert_results = trainer.evaluate(eval_dataset=full_collected_tokenized)\n",
    "    transferability_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"Roberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/roberta.txt\", \"w\") as f:\n",
    "    f.write(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")\n",
    "    f.write(f\"Roberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c680208-94f0-49f9-8497-168959c956a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_roberta_eng\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=eng_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=eng_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta_eng.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f64b97-7187-4a8f-9f33-2d265315b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ml/best_hparams/roberta_eng.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_eng\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "best_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594acee-3f0a-4c34-8d5d-67a3e53ce93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "transferability_acc = np.zeros(3)\n",
    "transferability_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=eng_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=eng_tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "    mbert_results = trainer.evaluate(eval_dataset=full_collected_tokenized)\n",
    "    transferability_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    transferability_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "transferability_acc *= 100\n",
    "transferability_f1 *= 100\n",
    "\n",
    "print(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "print(f\"Roberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/roberta_eng.txt\", \"w\") as f:\n",
    "    f.write(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")\n",
    "    f.write(f\"Roberta Transferability: {np.mean(transferability_acc):.4f}% +/- {np.std(transferability_acc):.4f} Weighted F1: {np.mean(transferability_f1):.4f}% +/- {np.std(transferability_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80856b-6b8d-40ca-a91b-69cae5919bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/trained_roberta_realworld\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=collected_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=collected_tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=24\n",
    ")\n",
    "\n",
    "with open(\"ml/best_hparams/roberta_realworld.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(best_trial.hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a9c3e-f8f7-4eee-83b9-c05ab8dd62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ml/best_hparams/roberta_realworld.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_realworld\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "best_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280a0e8-760a-48d1-91a7-3ff088cb60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveness_acc = np.zeros(3)\n",
    "effectiveness_f1 = np.zeros(3)\n",
    "\n",
    "for i in range(3):\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=collected_tokenized_dataset[\"train\"],\n",
    "        eval_dataset=collected_tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    mbert_results = trainer.evaluate()\n",
    "    effectiveness_acc[i] = mbert_results[\"eval_accuracy\"]\n",
    "    effectiveness_f1[i] = mbert_results[\"eval_f1\"]\n",
    "\n",
    "# Convert to percentage\n",
    "effectiveness_acc *= 100\n",
    "effectiveness_f1 *= 100\n",
    "\n",
    "print(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\")\n",
    "\n",
    "with open(\"./ml/effectiveness/roberta_realworld.txt\", \"w\") as f:\n",
    "    f.write(f\"Roberta Accuracy: {np.mean(effectiveness_acc):.4f}% +/- {np.std(effectiveness_acc):.4f} Weighted F1: {np.mean(effectiveness_f1):.4f}% +/- {np.std(effectiveness_f1):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e8de24-bea8-4bd8-a0e0-4fc2c893776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10276\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1929\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1929' max='1929' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1929/1929 11:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>0.431684</td>\n",
       "      <td>0.866793</td>\n",
       "      <td>0.863813</td>\n",
       "      <td>0.862271</td>\n",
       "      <td>0.863813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.365479</td>\n",
       "      <td>0.891852</td>\n",
       "      <td>0.887549</td>\n",
       "      <td>0.887512</td>\n",
       "      <td>0.887549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.384069</td>\n",
       "      <td>0.893583</td>\n",
       "      <td>0.892996</td>\n",
       "      <td>0.893012</td>\n",
       "      <td>0.892996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_realworld/checkpoint-643\n",
      "Configuration saved in ml/results/roberta_realworld/checkpoint-643/config.json\n",
      "Model weights saved in ml/results/roberta_realworld/checkpoint-643/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_realworld/checkpoint-643/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_realworld/checkpoint-643/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_realworld/checkpoint-1286\n",
      "Configuration saved in ml/results/roberta_realworld/checkpoint-1286/config.json\n",
      "Model weights saved in ml/results/roberta_realworld/checkpoint-1286/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_realworld/checkpoint-1286/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_realworld/checkpoint-1286/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2570\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ml/results/roberta_realworld/checkpoint-1929\n",
      "Configuration saved in ml/results/roberta_realworld/checkpoint-1929/config.json\n",
      "Model weights saved in ml/results/roberta_realworld/checkpoint-1929/pytorch_model.bin\n",
      "tokenizer config file saved in ml/results/roberta_realworld/checkpoint-1929/tokenizer_config.json\n",
      "Special tokens file saved in ml/results/roberta_realworld/checkpoint-1929/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ml/results/roberta_realworld/checkpoint-1286 (score: 0.3654787540435791).\n",
      "Saving model checkpoint to ./ml/trained_realworld_roberta_model\n",
      "Configuration saved in ./ml/trained_realworld_roberta_model/config.json\n",
      "Model weights saved in ./ml/trained_realworld_roberta_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./ml/trained_realworld_roberta_model/tokenizer_config.json\n",
      "Special tokens file saved in ./ml/trained_realworld_roberta_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ml/best_hparams/roberta_realworld.txt\") as f:\n",
    "    for line in f:\n",
    "        best_hparams = json.loads(line)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ml/results/roberta_realworld\",\n",
    "    learning_rate=best_hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=6, id2label=id2label, label2id=label2id)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=collected_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=collected_tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./ml/trained_realworld_roberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a75614be-7594-474c-9249-870264178277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=collected_tokenized_dataset[\"test\"],\n",
    "    metric=evaluate.combine([\"BucketHeadP65/confusion_matrix\", \"accuracy\"]),\n",
    "    label_mapping=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725d6a44-d7bf-4e88-9ba3-dd6fc44747c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[445,   6,   1,  19,   4,   6],\n",
       "       [  6, 558,   0,   3,   2,   0],\n",
       "       [  5,   1, 196,  11,  43,  15],\n",
       "       [  4,   3,   0, 385,  39,   8],\n",
       "       [  6,   3,   8,  55, 486,   8],\n",
       "       [  3,   2,   0,  14,  14, 211]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"confusion_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47643dc-e55b-4525-971c-c7dcdf1e3914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
