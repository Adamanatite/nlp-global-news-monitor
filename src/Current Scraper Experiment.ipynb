{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d0f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sources.txt\") as f:\n",
    "    # Parse source file\n",
    "    sources = {}\n",
    "    \n",
    "    current_key = None\n",
    "    for line in f:\n",
    "        data = line.strip().split(\" \")\n",
    "        if len(data) == 1:\n",
    "            current_key = data[0]\n",
    "            continue\n",
    "        sources[current_key] = sources.get(current_key, []) + [{\"name\": \" \".join(data[:-5]),\"url\": data[-5], \"country\": data[-4], \"articles\": data[-3:]}]\n",
    "        \n",
    "from newspaper import Article\n",
    "from newsplease import NewsPlease\n",
    "\n",
    "def article_parse(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faabe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Source, Country, URL, NP3K_time, n-p time\n",
    "def run_experiment():\n",
    "    with open(\"article_results.csv\", \"w\") as of:\n",
    "        with open(\"errors.txt\", \"w\") as err_f:\n",
    "            of.write(\"Language,Source,Country,URL,Newspaper3k time,news-please time\\n\")\n",
    "            for lang in sources.keys():\n",
    "                for source in sources[lang]:\n",
    "                    print(source[\"name\"])\n",
    "                    #Initialise time for source\n",
    "                    np3k_time = 0.0\n",
    "                    newsplease_time = 0.0\n",
    "                    np3k_errors = 0\n",
    "                    newsplease_errors = 0\n",
    "\n",
    "                    for url in source[\"articles\"]:\n",
    "                        #Time newspaper3k and news-please article extraction\n",
    "                        try:\n",
    "                            current_time = %timeit -o -r3 article_parse(url)\n",
    "                            np3k_time = np3k_time + current_time.average\n",
    "                        except Exception as e:\n",
    "                            err_f.write(\"Newspaper3k \" + url + \" \" + str(e) + \"\\n\")\n",
    "                            np3k_errors = np3k_errors + 1\n",
    "                        try:\n",
    "                            current_time = %timeit -o -r3 article = NewsPlease.from_url(url)\n",
    "                            newsplease_time = newsplease_time + current_time.average\n",
    "                        except Exception as e:\n",
    "                            err_f.write(\"news-please \" + url + \" \" + str(e) + \"\\n\")\n",
    "                            newsplease_errors = newsplease_errors + 1\n",
    "\n",
    "                    #Average out time per article (put -1 if it failed)\n",
    "                    try:\n",
    "                        np3k_time = np3k_time / (len(source[\"articles\"]) - np3k_errors)\n",
    "                    except ZeroDivisionError:\n",
    "                        np3k_time = -1.0\n",
    "                    try:\n",
    "                        newsplease_time = (newsplease_time / len(source[\"articles\"]) - newsplease_errors)\n",
    "                    except ZeroDivisionError:\n",
    "                        newsplease_time = -1.0\n",
    "                    #Write line to CSV\n",
    "                    of.write(lang + \",\" + source[\"name\"] + \",\" + source[\"country\"] + \",\" + source[\"url\"] + \",\" + \"{:.2f}\".format(np3k_time*1_000) + \",\" + \"{:.2f}\".format(newsplease_time*1_000) + \"\\n\")\n",
    "    print(\"Wrote results into article_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate results into table based on language\n",
    "def aggregate_results():\n",
    "    aggregated = {}\n",
    "    with open(\"article_results.csv\") as f:\n",
    "        with open(\"aggregated_results.csv\",\"w\") as of:\n",
    "            of.write(\"Language,N,NP3k Correct,np correct,NP3k time,np time\\n\")\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                #Remove first line\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    continue\n",
    "\n",
    "                #Parse data\n",
    "                data = line.strip().split(\",\")\n",
    "                aggregated[data[0]] = aggregated.get(data[0], {})\n",
    "                lang = aggregated[data[0]]\n",
    "                #Initialise dictionary values\n",
    "                lang[\"NP3K_correct\"] = lang.get(\"NP3K_correct\", 0)\n",
    "                lang[\"np_correct\"] = lang.get(\"np_correct\", 0)\n",
    "                lang[\"NP3K_time\"] = lang.get(\"NP3K_time\", 0.0)\n",
    "                lang[\"np_time\"] = lang.get(\"np_time\", 0.0)\n",
    "                lang[\"N\"] = lang.get(\"N\", 0) + 1\n",
    "                #Add to correct if correct (nonnegative time)\n",
    "                if(float(data[4]) >= 0):\n",
    "                    lang[\"NP3K_correct\"] += 1\n",
    "                    lang[\"NP3K_time\"] += float(data[4])\n",
    "                if(float(data[5]) >= 0):\n",
    "                    lang[\"np_correct\"] += 1\n",
    "                    lang[\"np_time\"] += float(data[5])  \n",
    "            #Add data per language to new file\n",
    "            total_n = 0\n",
    "            total_np3k = 0\n",
    "            total_np = 0\n",
    "            total_np3k_time = 0.0\n",
    "            total_np_time = 0.0\n",
    "\n",
    "            for language in aggregated.keys():\n",
    "                #Calculate average times for language\n",
    "                average_np3k = aggregated[language][\"NP3K_time\"] / aggregated[language][\"NP3K_correct\"]\n",
    "                average_np = aggregated[language][\"np_time\"] / aggregated[language][\"np_correct\"]\n",
    "                #Write language row in file\n",
    "                of.write(language+\",\"+str(aggregated[language][\"N\"])+\",\"+str(aggregated[language][\"NP3K_correct\"])+\",\"+str(aggregated[language][\"np_correct\"])+\",\"+str(average_np3k)+\",\"+str(average_np)+\"\\n\")\n",
    "                #Calculate total statistics\n",
    "                total_n += aggregated[language][\"N\"]\n",
    "                total_np3k += aggregated[language][\"NP3K_correct\"]\n",
    "                total_np += aggregated[language][\"np_correct\"]\n",
    "                total_np3k_time = aggregated[language][\"NP3K_time\"]\n",
    "                total_np_time = aggregated[language][\"np_time\"]\n",
    "\n",
    "            #Do final row of table for total\n",
    "            total_average_np3k = total_np3k_time / total_np3k\n",
    "            total_average_np = total_np_time / total_np\n",
    "            of.write(\"Total,\"+str(total_n)+\",\"+str(total_np3k)+\",\"+str(total_np)+\",\"+str(total_average_np3k)+\",\"+str(total_average_np)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a913d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\\\n",
    "\n",
    "def measure_country(lang):\n",
    "    with open(\"article_results_\" + lang + \".csv\", \"w\") as of:\n",
    "        with open(\"errors_\"+lang+\".txt\", \"w\") as err_f:\n",
    "            for source in sources[lang]:\n",
    "                print(source[\"name\"])\n",
    "                #Initialise time for source\n",
    "                np3k_time = 0.0\n",
    "                newsplease_time = 0.0\n",
    "                np3k_errors = 0\n",
    "                newsplease_errors = 0\n",
    "\n",
    "                for url in source[\"articles\"]:\n",
    "                    #Time newspaper3k and news-please article extraction\n",
    "                    try:\n",
    "                        current_time = %timeit -o -r3 article_parse(url)\n",
    "                        np3k_time = np3k_time + current_time.average\n",
    "                    except Exception as e:\n",
    "                        err_f.write(\"Newspaper3k \" + url + \" \" + str(e) + \"\\n\")\n",
    "                        np3k_errors = np3k_errors + 1\n",
    "                    try:\n",
    "                        current_time = %timeit -o -r3 article = NewsPlease.from_url(url)\n",
    "                        newsplease_time = newsplease_time + current_time.average\n",
    "                    except Exception as e:\n",
    "                        err_f.write(\"news-please \" + url + \" \" + str(e) + \"\\n\")\n",
    "                        newsplease_errors = newsplease_errors + 1\n",
    "\n",
    "                #Average out time per article (put -1 if it failed)\n",
    "                try:\n",
    "                    np3k_time = np3k_time / (len(source[\"articles\"]) - np3k_errors)\n",
    "                except ZeroDivisionError:\n",
    "                    np3k_time = -1.0\n",
    "                try:\n",
    "                    newsplease_time = (newsplease_time / len(source[\"articles\"]) - newsplease_errors)\n",
    "                except ZeroDivisionError:\n",
    "                    newsplease_time = -1.0\n",
    "                #Write line to CSV\n",
    "                of.write(lang + \",\" + source[\"name\"] + \",\" + source[\"country\"] + \",\" + source[\"url\"] + \",\" + \"{:.2f}\".format(np3k_time*1_000) + \",\" + \"{:.2f}\".format(newsplease_time*1_000) + \"\\n\")\n",
    "                #Clear output\n",
    "                clear_output(wait=True)\n",
    "    print(\"Wrote results into article_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055997b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITV News\n",
      "370 ms ± 9.29 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "measure_country(\"ENG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8547b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_countries():\n",
    "    of.write(\"Language,Source,Country,URL,Newspaper3k time,news-please time\\n\")\n",
    "    with open(\"article_results.csv\", \"w\") as of:\n",
    "        of.write()\n",
    "        for key in sources.keys():\n",
    "            with open(\"article_results_\"+code+\".csv\", \"w\") as f:\n",
    "                for line in f:\n",
    "                    of.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d56efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "not a 200 response: 403\n"
     ]
    },
    {
     "ename": "ArticleException",
     "evalue": "You must `download()` an article first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mNewsPlease\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mENG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test\u001b[38;5;241m.\u001b[39mtitle\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newsplease\\__init__.py:110\u001b[0m, in \u001b[0;36mNewsPlease.from_url\u001b[1;34m(url, timeout)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_url\u001b[39m(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Crawls the article from the url and extracts relevant information.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    :param url:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    :rtype: NewsArticle, None\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     articles \u001b[38;5;241m=\u001b[39m \u001b[43mNewsPlease\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43murl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m articles\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m articles[url]\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newsplease\\__init__.py:134\u001b[0m, in \u001b[0;36mNewsPlease.from_urls\u001b[1;34m(urls, timeout)\u001b[0m\n\u001b[0;32m    132\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    133\u001b[0m     html \u001b[38;5;241m=\u001b[39m SimpleCrawler\u001b[38;5;241m.\u001b[39mfetch_url(url, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m--> 134\u001b[0m     results[url] \u001b[38;5;241m=\u001b[39m \u001b[43mNewsPlease\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     results \u001b[38;5;241m=\u001b[39m SimpleCrawler\u001b[38;5;241m.\u001b[39mfetch_urls(urls)\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newsplease\\__init__.py:95\u001b[0m, in \u001b[0;36mNewsPlease.from_html\u001b[1;34m(html, url, download_date, fetch_images)\u001b[0m\n\u001b[0;32m     93\u001b[0m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m download_date\n\u001b[0;32m     94\u001b[0m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodified_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m tmp_article \u001b[38;5;241m=\u001b[39m ExtractedInformationStorage\u001b[38;5;241m.\u001b[39mextract_relevant_info(item)\n\u001b[0;32m     98\u001b[0m final_article \u001b[38;5;241m=\u001b[39m ExtractedInformationStorage\u001b[38;5;241m.\u001b[39mconvert_to_class(tmp_article)\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newsplease\\pipeline\\extractor\\article_extractor.py:63\u001b[0m, in \u001b[0;36mExtractor.extract\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     60\u001b[0m article_candidates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m extractor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor_list:\n\u001b[1;32m---> 63\u001b[0m     article_candidate \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     article_candidates\u001b[38;5;241m.\u001b[39mappend(article_candidate)\n\u001b[0;32m     66\u001b[0m article_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleaner\u001b[38;5;241m.\u001b[39mclean(article_candidates)\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newsplease\\pipeline\\extractor\\extractors\\newspaper_extractor.py:33\u001b[0m, in \u001b[0;36mNewspaperExtractor.extract\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     31\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_article_kwargs())\n\u001b[0;32m     32\u001b[0m article\u001b[38;5;241m.\u001b[39mset_html(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspider_response\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbody)\n\u001b[1;32m---> 33\u001b[0m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m article_candidate\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mtitle\n\u001b[0;32m     35\u001b[0m article_candidate\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mmeta_description\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newspaper\\article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc)\n",
      "File \u001b[1;32mD:\\Workspace\\NLP Global News Monitor\\src\\.env\\lib\\site-packages\\newspaper\\article.py:529\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"Parse ArticleDownloadState -> log readable status\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m-> maybe throw ArticleException\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mNOT_STARTED:\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must `download()` an article first!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[1;31mArticleException\u001b[0m: You must `download()` an article first!"
     ]
    }
   ],
   "source": [
    "test = NewsPlease.from_url(sources[\"ENG\"][5][\"articles\"][2])\n",
    "test.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb12300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.itv.com/news/2022-10-23/tory-mps-to-make-their-choice-for-leader-after-johnson-withdraws\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sunak could be declared next PM within hours after Johnson abandons leadership bid'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = article_parse(sources[\"ENG\"][6][\"articles\"][0])\n",
    "print(sources[\"ENG\"][6][\"articles\"][0])\n",
    "test.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdf8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG\n",
      "AP News\n",
      "Reuters\n",
      "BBC News\n",
      "Stat News\n",
      "CNN\n",
      "NY Times\n",
      "ITV News\n",
      "The Guardian\n",
      "CBC News\n",
      "Global News\n",
      "Telegraph\n",
      "ABC News\n",
      "9News\n",
      "Scoop\n",
      "Stuff.co\n",
      "RTE News\n",
      "Jamaica Observer\n",
      "Trinidad express\n",
      "Loop Trinidad\n",
      "The East African\n",
      "Monitor\n",
      "WHO\n",
      "CDC\n",
      "The BMJ\n",
      "Reliefweb\n",
      "New Scientist\n",
      "Medical Independent\n",
      "CMAJ News\n",
      "FRA\n",
      "Le Figaro\n",
      "Le Monde\n",
      "Liberation\n",
      "L'Express\n",
      "La Presse\n",
      "Le Devoir\n",
      "Le Nouvelliste\n",
      "La presse.tn\n",
      "Mediacongo\n",
      "Congo Independent\n",
      "Le Matin du Sahara er du Maghreb\n",
      "ESP\n",
      "El Universal\n",
      "El Mundo\n",
      "ABC\n",
      "El Pais\n",
      "La Nacion\n",
      "El nacional\n",
      "Pulso\n",
      "La Jornada\n",
      "El Tiempo\n",
      "El Espectador\n",
      "ZHO\n",
      "Sina\n",
      "Huanqiu\n",
      "Beijing News\n",
      "China Daily\n",
      "Ming Pao\n",
      "Liberty\n",
      "China Times\n",
      "Lianhe Zaobao\n",
      "Health news network\n",
      "RUS\n",
      "TASS\n",
      "RIA Novosti\n",
      "Interfax\n",
      "REGNUM\n",
      "Komsomolskaya Pravda\n",
      "New times\n",
      "Nur.kz\n",
      "DSNews\n",
      "BELTA\n",
      "POR\n",
      "Publico\n",
      "DiÃ¡rio de NotÃ­cias\n",
      "Jornal de NotÃ­cias\n",
      "Folha De S.Paulo\n",
      "Rede Globo Noticias\n",
      "Terra\n",
      "O Democrata\n",
      "Radio Jovem\n",
      "Expresso das Ilhas\n",
      "Jornal Noticias\n",
      "IND\n",
      "Kompas\n",
      "Republika\n",
      "Jawa Pos\n",
      "Pikiran Rakyat\n",
      "Bali post\n",
      "SWA\n",
      "Habari Leo\n",
      "MTanzania\n",
      "IPP Media\n",
      "Global Publishers\n",
      "Tuko\n",
      "Taiga Leo\n",
      "BBC News Swahili\n",
      "KOR\n",
      "Dong-a Ilbo\n",
      "Joongang Ilbo\n",
      "Chosun Iibo\n",
      "Yonhap News Agency\n",
      "Kyunghyang Shinmun\n",
      "Ohmynews\n",
      "ARA\n",
      "Al Jazeera\n",
      "Al-ahram\n",
      "Al Waft\n",
      "Elkhabar\n",
      "Al Bayan\n",
      "EAl-Akhbar\n",
      "Alquds\n",
      "Erm News\n",
      "Alintibaha\n",
      "Shafaq News\n",
      "Assabah\n",
      "Okaz\n",
      "CNN Arabic\n",
      "BBC Arabic\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_output.txt\", \"w\", encoding=\"utf-8\") as of:\n",
    "    for lang in sources.keys():\n",
    "        print(lang)\n",
    "        for source in sources[lang]:\n",
    "            print(source[\"name\"])\n",
    "            for article in source[\"articles\"]:\n",
    "                try:\n",
    "                    test = article_parse(article)\n",
    "                    if test:\n",
    "                        title = test.title\n",
    "                    else:\n",
    "                        title = \"Didn't parse\"\n",
    "                except:\n",
    "                    title = \"Error\"\n",
    "                of.write(source[\"name\"] + \" \" + title + \"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f0d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
