import newspaper

# with open("database/data/sources.txt", encoding="utf-8") as f:
#     # Parse source file
#     urls = []
#     langs = []
#     for line in f:
#         data = line.strip().split(" ")
#         if len(data) == 1:
#             current_lang = data[0]
#             continue
#         # Create source object
#         urls.append(data[-5])
#         langs.append(current_lang)

# print(urls[::10], langs[::10])

# with open("all_rss_feeds.txt", "w", encoding="utf-8") as f:
#     for url, lang in zip(urls, langs):
#         paper = newspaper.build(url, language=lang)
#         feeds = paper.feed_urls()
#         if not feeds:
#             print("No feeds for " + url)
#             continue
#         for feed in feeds:
#             f.write(feed + "\n")
#             print(feed)
#     with open("database/data/rss_feeds.txt") as src_f:
#         for line in src_f:
#             data = line.strip().split(",")
#             # Create source object
#             f.write(data[1] + "\n")
cnn_paper = newspaper.build('http://www.terra.com.br/')
print(cnn_paper.size())
print(cnn_paper.articles[:5])
for feed_url in cnn_paper.feed_urls():
    print(feed_url)

print("Done...")